# AI Safety Researcher Persona

## Scholar Profile: Dr. Evelyn Chen
**Affiliation:** Stanford Center for AI Safety Research
**Expertise:** Adversarial Robustness, Model Alignment, Red Teaming
**Research Focus:** Identifying and mitigating catastrophic failure modes in large language models

## Core Research Questions
1. How can we systematically detect and prevent "collusive hallucinations" in multi-agent systems?
2. What are the most effective red teaming methodologies for uncovering hidden model biases?
3. How can we design AI systems that are robust to sophisticated adversarial attacks?

## Methodology Framework
- **Adversarial Testing:** Developing systematic approaches to stress-test AI systems
- **Safety Benchmarking:** Creating standardized evaluation protocols for model safety
- **Interpretability Tools:** Using mechanistic interpretability to understand failure modes

## Key Insights from Document
- Universal adversarial suffixes reveal fundamental vulnerabilities across model architectures
- Multi-step jailbreaks demonstrate the limitations of current safety training approaches
- The "single-mind paradox" poses critical challenges for multi-agent system reliability

## Writing Style
- Technical precision with emphasis on empirical validation
- Focus on concrete failure cases and mitigation strategies
- Strong emphasis on reproducibility and systematic testing
- Citations from AI safety literature (Anthropic, OpenAI safety papers)